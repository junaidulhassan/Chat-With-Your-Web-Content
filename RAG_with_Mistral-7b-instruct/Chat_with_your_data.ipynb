{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T17:30:05.149081Z","iopub.execute_input":"2024-05-07T17:30:05.149402Z","iopub.status.idle":"2024-05-07T17:30:06.218438Z","shell.execute_reply.started":"2024-05-07T17:30:05.149375Z","shell.execute_reply":"2024-05-07T17:30:06.217511Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install langchain==0.1.6","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:31:28.209191Z","iopub.execute_input":"2024-05-07T17:31:28.209917Z","iopub.status.idle":"2024-05-07T17:31:28.213870Z","shell.execute_reply.started":"2024-05-07T17:31:28.209884Z","shell.execute_reply":"2024-05-07T17:31:28.212958Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:32:44.955493Z","iopub.execute_input":"2024-05-07T17:32:44.955814Z","iopub.status.idle":"2024-05-07T17:32:44.959596Z","shell.execute_reply.started":"2024-05-07T17:32:44.955788Z","shell.execute_reply":"2024-05-07T17:32:44.958612Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#!pip install chromadb","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:32:35.011697Z","iopub.execute_input":"2024-05-07T17:32:35.012388Z","iopub.status.idle":"2024-05-07T17:32:35.017119Z","shell.execute_reply.started":"2024-05-07T17:32:35.012350Z","shell.execute_reply":"2024-05-07T17:32:35.015898Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:33.989743Z","iopub.execute_input":"2024-05-07T17:33:33.990109Z","iopub.status.idle":"2024-05-07T17:33:33.994390Z","shell.execute_reply.started":"2024-05-07T17:33:33.990065Z","shell.execute_reply":"2024-05-07T17:33:33.993471Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Import LLM ","metadata":{}},{"cell_type":"code","source":"import warnings as wn\nwn.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:35.243995Z","iopub.execute_input":"2024-05-07T17:33:35.244868Z","iopub.status.idle":"2024-05-07T17:33:35.248870Z","shell.execute_reply.started":"2024-05-07T17:33:35.244832Z","shell.execute_reply":"2024-05-07T17:33:35.247958Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:36.559028Z","iopub.execute_input":"2024-05-07T17:33:36.559723Z","iopub.status.idle":"2024-05-07T17:33:42.709819Z","shell.execute_reply.started":"2024-05-07T17:33:36.559689Z","shell.execute_reply":"2024-05-07T17:33:42.708871Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\nfrom transformers import TextStreamer\nfrom transformers import Pipeline,pipeline","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:44.273826Z","iopub.execute_input":"2024-05-07T17:33:44.274728Z","iopub.status.idle":"2024-05-07T17:33:58.068017Z","shell.execute_reply.started":"2024-05-07T17:33:44.274690Z","shell.execute_reply":"2024-05-07T17:33:58.067217Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2024-05-07 17:33:47.149831: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-07 17:33:47.149939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-07 17:33:47.314195: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.llms.huggingface_pipeline import HuggingFacePipeline","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:58.069839Z","iopub.execute_input":"2024-05-07T17:33:58.070782Z","iopub.status.idle":"2024-05-07T17:33:58.533095Z","shell.execute_reply.started":"2024-05-07T17:33:58.070743Z","shell.execute_reply":"2024-05-07T17:33:58.532316Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:58.534266Z","iopub.execute_input":"2024-05-07T17:33:58.534563Z","iopub.status.idle":"2024-05-07T17:33:58.538733Z","shell.execute_reply.started":"2024-05-07T17:33:58.534536Z","shell.execute_reply":"2024-05-07T17:33:58.537726Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_path='/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path\n)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    trust_remote_code=True,\n    offload_folder='offload',\n    device_map='auto',\n    torch_dtype=torch.float16\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:33:59.872655Z","iopub.execute_input":"2024-05-07T17:33:59.873017Z","iopub.status.idle":"2024-05-07T17:35:42.879828Z","shell.execute_reply.started":"2024-05-07T17:33:59.872986Z","shell.execute_reply":"2024-05-07T17:35:42.878838Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31f2a83b6da94e0aa8ae89e047a2de8e"}},"metadata":{}}]},{"cell_type":"code","source":"streamer = TextStreamer(\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:41:24.749174Z","iopub.execute_input":"2024-05-07T17:41:24.749537Z","iopub.status.idle":"2024-05-07T17:41:24.754034Z","shell.execute_reply.started":"2024-05-07T17:41:24.749506Z","shell.execute_reply":"2024-05-07T17:41:24.753125Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model = model,\n    tokenizer = tokenizer,\n    max_new_tokens = 500,\n#     streamer = streamer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:05:32.111955Z","iopub.execute_input":"2024-05-07T18:05:32.112344Z","iopub.status.idle":"2024-05-07T18:05:32.117296Z","shell.execute_reply.started":"2024-05-07T18:05:32.112313Z","shell.execute_reply":"2024-05-07T18:05:32.116241Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=pipe)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:05:33.646399Z","iopub.execute_input":"2024-05-07T18:05:33.646765Z","iopub.status.idle":"2024-05-07T18:05:33.652521Z","shell.execute_reply.started":"2024-05-07T18:05:33.646734Z","shell.execute_reply":"2024-05-07T18:05:33.651508Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"llm(\"Who is elon?\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:02:07.201395Z","iopub.execute_input":"2024-05-07T18:02:07.202247Z","iopub.status.idle":"2024-05-07T18:02:13.106602Z","shell.execute_reply.started":"2024-05-07T18:02:07.202201Z","shell.execute_reply":"2024-05-07T18:02:13.105682Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Who is elon?\nUser 1: Elon Musk is a billionaire entrepreneur who has founded multiple companies, including SpaceX, Tesla, Neuralink, and The Boring Company. He is known for his ambitious plans to revolutionize various industries, such as space travel, electric cars, and artificial intelligence. Musk has also been involved in several high-profile projects, including the development of the Hyperloop and the colonization of Mars.</s>\n","output_type":"stream"},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"'\\nUser 1: Elon Musk is a billionaire entrepreneur who has founded multiple companies, including SpaceX, Tesla, Neuralink, and The Boring Company. He is known for his ambitious plans to revolutionize various industries, such as space travel, electric cars, and artificial intelligence. Musk has also been involved in several high-profile projects, including the development of the Hyperloop and the colonization of Mars.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Use Langchain","metadata":{}},{"cell_type":"code","source":"# import packages from langchains for llm\n\nfrom langchain.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.embeddings import HuggingFaceEmbeddings","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:05:47.350295Z","iopub.execute_input":"2024-05-07T18:05:47.350976Z","iopub.status.idle":"2024-05-07T18:05:47.356329Z","shell.execute_reply.started":"2024-05-07T18:05:47.350941Z","shell.execute_reply":"2024-05-07T18:05:47.355237Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"# Scrap Data from Website","metadata":{}},{"cell_type":"code","source":"# import python packages to scrap data from sites\n\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.document_loaders import PyPDFLoader","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:05:49.846822Z","iopub.execute_input":"2024-05-07T18:05:49.847239Z","iopub.status.idle":"2024-05-07T18:05:49.851948Z","shell.execute_reply.started":"2024-05-07T18:05:49.847199Z","shell.execute_reply":"2024-05-07T18:05:49.850814Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"web_path = 'https://www.upwork.com/freelancers/~015e7fe10fdf1c7f4f'","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:20:14.188810Z","iopub.execute_input":"2024-05-07T18:20:14.189670Z","iopub.status.idle":"2024-05-07T18:20:14.193711Z","shell.execute_reply.started":"2024-05-07T18:20:14.189633Z","shell.execute_reply":"2024-05-07T18:20:14.192624Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"# now load the document\n\nloader = WebBaseLoader(\n    web_path = web_path,\n    header_template = None,\n    verify_ssl = True,\n    proxies = None,\n    continue_on_failure = False,\n    requests_per_second = 2,\n    default_parser = 'html.parser'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:20:15.572696Z","iopub.execute_input":"2024-05-07T18:20:15.573081Z","iopub.status.idle":"2024-05-07T18:20:15.581129Z","shell.execute_reply.started":"2024-05-07T18:20:15.573049Z","shell.execute_reply":"2024-05-07T18:20:15.580190Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"docs = loader.load()\ntxt_data = docs[0].page_content","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:20:17.556411Z","iopub.execute_input":"2024-05-07T18:20:17.556829Z","iopub.status.idle":"2024-05-07T18:20:17.686814Z","shell.execute_reply.started":"2024-05-07T18:20:17.556796Z","shell.execute_reply":"2024-05-07T18:20:17.686104Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"len(txt_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:20:19.195776Z","iopub.execute_input":"2024-05-07T18:20:19.196216Z","iopub.status.idle":"2024-05-07T18:20:19.202346Z","shell.execute_reply.started":"2024-05-07T18:20:19.196187Z","shell.execute_reply":"2024-05-07T18:20:19.201431Z"},"trusted":true},"execution_count":152,"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"57"},"metadata":{}}]},{"cell_type":"code","source":"print(txt_data[0:500])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:20:22.289973Z","iopub.execute_input":"2024-05-07T18:20:22.290326Z","iopub.status.idle":"2024-05-07T18:20:22.295459Z","shell.execute_reply.started":"2024-05-07T18:20:22.290298Z","shell.execute_reply":"2024-05-07T18:20:22.294457Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"Just a moment...Enable JavaScript and cookies to continue\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Splitter","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import CharacterTextSplitter","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:06:41.062760Z","iopub.execute_input":"2024-05-07T18:06:41.063146Z","iopub.status.idle":"2024-05-07T18:06:41.067955Z","shell.execute_reply.started":"2024-05-07T18:06:41.063113Z","shell.execute_reply":"2024-05-07T18:06:41.066928Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# now define the chunks and overlap\n\nchunks_size = 500\nchunks_overlap = 40\n\nrec_splitter = RecursiveCharacterTextSplitter(\n    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n    chunk_size=chunks_size,\n    chunk_overlap=chunks_overlap,\n    is_separator_regex=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:06:42.329391Z","iopub.execute_input":"2024-05-07T18:06:42.329754Z","iopub.status.idle":"2024-05-07T18:06:42.334826Z","shell.execute_reply.started":"2024-05-07T18:06:42.329722Z","shell.execute_reply":"2024-05-07T18:06:42.333924Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"rec_splitter.split_text(txt_data[0:1000])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:02:50.609546Z","iopub.execute_input":"2024-05-07T18:02:50.609935Z","iopub.status.idle":"2024-05-07T18:02:50.617381Z","shell.execute_reply.started":"2024-05-07T18:02:50.609903Z","shell.execute_reply":"2024-05-07T18:02:50.616299Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"['What is LangChain? Getting Started with LangChain | DataStaxProductsPricingStoriesResourcesDocsContact UsSign InSign InTry For FreeHANDS-ON WORKSHOP MAY 16: Learn to build a multimodal GenAI recommendation appBack to ResourcesGuide • Nov 09, 2023What is LangChain? Understanding Key Components and PrinciplesLangChain aids in implementing the Retrieval Augmented Generation (RAG) pattern in applications, simplifying data retrieval from various sources. How does this contribute to more contextually',\n 'this contribute to more contextually aware AI applications?Sign Up for AstraDavid DierufAI Developer AdvocateWhat is LangChain?LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with Large Language Models (LLMs). It offers features for data communication, generation of vector embeddings, and simplifies the interaction with LLMs, making it efficient for AI developers.When you refer to an application as “AI”',\n 'you refer to an application as “AI” that usually means it includes interactio']"},"metadata":{}}]},{"cell_type":"code","source":"# now try the Simple charater splitter\n\nsplitter = CharacterTextSplitter(\n    separator='\\n',\n    chunk_size=chunks_size,\n    chunk_overlap = chunks_overlap\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:02:52.561729Z","iopub.execute_input":"2024-05-07T18:02:52.562127Z","iopub.status.idle":"2024-05-07T18:02:52.567124Z","shell.execute_reply.started":"2024-05-07T18:02:52.562096Z","shell.execute_reply":"2024-05-07T18:02:52.566208Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"splitter.split_text(txt_data[0: 1500])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:02:54.294709Z","iopub.execute_input":"2024-05-07T18:02:54.295526Z","iopub.status.idle":"2024-05-07T18:02:54.301380Z","shell.execute_reply.started":"2024-05-07T18:02:54.295488Z","shell.execute_reply":"2024-05-07T18:02:54.300428Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"['What is LangChain? Getting Started with LangChain | DataStaxProductsPricingStoriesResourcesDocsContact UsSign InSign InTry For FreeHANDS-ON WORKSHOP MAY 16: Learn to build a multimodal GenAI recommendation appBack to ResourcesGuide • Nov 09, 2023What is LangChain? Understanding Key Components and PrinciplesLangChain aids in implementing the Retrieval Augmented Generation (RAG) pattern in applications, simplifying data retrieval from various sources. How does this contribute to more contextually aware AI applications?Sign Up for AstraDavid DierufAI Developer AdvocateWhat is LangChain?LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with Large Language Models (LLMs). It offers features for data communication, generation of vector embeddings, and simplifies the interaction with LLMs, making it efficient for AI developers.When you refer to an application as “AI” that usually means it includes interactions with a learning model (like a large language model LLM). The [not so] interesting fact is that the use of an LLM is not actually what makes the app intelligent. It’s the use of a neural network in real time that makes it special. It just so happens that LLMs are built using neural networks. An AI application typically processes data in real-time. That means, that while it has a lot of pre-trained knowledge, it can take in data as it’s submitted to the application and give the LLM up-to-date i']"},"metadata":{}}]},{"cell_type":"code","source":"split = rec_splitter.split_documents(\n    docs\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:39.698882Z","iopub.execute_input":"2024-05-07T17:42:39.699257Z","iopub.status.idle":"2024-05-07T17:42:39.713093Z","shell.execute_reply.started":"2024-05-07T17:42:39.699226Z","shell.execute_reply":"2024-05-07T17:42:39.712068Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"embedding = HuggingFaceEmbeddings(\n    model_name='all-MiniLM-L6-v2'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:39.867748Z","iopub.execute_input":"2024-05-07T17:42:39.868035Z","iopub.status.idle":"2024-05-07T17:42:43.303890Z","shell.execute_reply.started":"2024-05-07T17:42:39.868010Z","shell.execute_reply":"2024-05-07T17:42:43.303032Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"082e9d4c5cb54e75bd45821e7e0bc792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a16d95280f4ecf984e296149b931b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ac9885ed9d49a7ba9e1fef04ec480c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b299be046e34be3aa91104e929e4079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab35e02b6dab41d6b7d5a6b770ce19c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763841e3c4a342baa2d07f559319c9cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bada8b430ba046458257e1c93e3a934c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021f292a88b44a3dbb8a5bd8d99499ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba8cf4997cc4991bfa3c369b95c2e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad04f3e7a01414db040c1afc2aacfe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221a8fec7a42412496239401896812de"}},"metadata":{}}]},{"cell_type":"code","source":"t1= \"I love dogs\"\nt2 = \"I love cats\"\nt3 = \"Study is very hard\"","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:45.388254Z","iopub.execute_input":"2024-05-07T17:42:45.388657Z","iopub.status.idle":"2024-05-07T17:42:45.393096Z","shell.execute_reply.started":"2024-05-07T17:42:45.388622Z","shell.execute_reply":"2024-05-07T17:42:45.392235Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"embd1 = embedding.embed_query(t1)\nembd2 = embedding.embed_query(t2)\nembd3 = embedding.embed_query(t3)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:45.661868Z","iopub.execute_input":"2024-05-07T17:42:45.662201Z","iopub.status.idle":"2024-05-07T17:42:45.922314Z","shell.execute_reply.started":"2024-05-07T17:42:45.662173Z","shell.execute_reply":"2024-05-07T17:42:45.921189Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"np.dot(embd1,embd2), np.dot(embd2,embd3)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:45.924023Z","iopub.execute_input":"2024-05-07T17:42:45.924324Z","iopub.status.idle":"2024-05-07T17:42:45.931212Z","shell.execute_reply.started":"2024-05-07T17:42:45.924297Z","shell.execute_reply":"2024-05-07T17:42:45.930219Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(0.7720984757242813, 0.06933481201584218)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Add Vector Databases","metadata":{}},{"cell_type":"code","source":"from langchain.vectorstores import Chroma\nfrom langchain.vectorstores import pinecone","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:48.286023Z","iopub.execute_input":"2024-05-07T17:42:48.286671Z","iopub.status.idle":"2024-05-07T17:42:48.294362Z","shell.execute_reply.started":"2024-05-07T17:42:48.286637Z","shell.execute_reply":"2024-05-07T17:42:48.293591Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#!chmod 664 /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:13:41.940246Z","iopub.execute_input":"2024-05-06T19:13:41.940948Z","iopub.status.idle":"2024-05-06T19:13:41.945042Z","shell.execute_reply.started":"2024-05-06T19:13:41.940915Z","shell.execute_reply":"2024-05-06T19:13:41.944172Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"db = Chroma.from_documents(\n    documents= split,\n    embedding=embedding,\n    collection_name='langchain',\n    persist_directory='docs/chroma/'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:42:51.786840Z","iopub.execute_input":"2024-05-07T17:42:51.787632Z","iopub.status.idle":"2024-05-07T17:42:53.873717Z","shell.execute_reply.started":"2024-05-07T17:42:51.787586Z","shell.execute_reply":"2024-05-07T17:42:53.872738Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"query = 'What is langchain?'","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:43:01.009546Z","iopub.execute_input":"2024-05-07T17:43:01.009920Z","iopub.status.idle":"2024-05-07T17:43:01.014075Z","shell.execute_reply.started":"2024-05-07T17:43:01.009888Z","shell.execute_reply":"2024-05-07T17:43:01.013218Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"db.similarity_search(\n    query=query,\n    k=3\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:43:02.391797Z","iopub.execute_input":"2024-05-07T17:43:02.392627Z","iopub.status.idle":"2024-05-07T17:43:02.416922Z","shell.execute_reply.started":"2024-05-07T17:43:02.392593Z","shell.execute_reply":"2024-05-07T17:43:02.416108Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='completion of the prompt.All the steps in the RAG pattern need to happen in succession. If anything errors along the way, the processing should stop. LangChain offers the “chain” construct to attach steps together in a specific way with a specific configuration. All LangChain libraries follow this chain construct to make it very easy to move steps around and create powerful processing pipelines.What are the Benefits of Using LangChain?Using LangChain offers several benefits in the realm of', metadata={'description': 'In this blog, we cover LangChain: what it is, how it works, examples, & its ability to enhance applications by combining LLMs with other computational sources.', 'language': 'en', 'source': 'https://www.datastax.com/guides/what-is-langchain', 'title': 'What is LangChain? Getting Started with LangChain | DataStax'}),\n Document(page_content='in LangChain are the decision-making components. They determine the best course of action based on the input, the context, and the available resources within the system.What are the Main Features of LangChain?Below is a summary of the most popular features of LangChain. This is not an exhaustive list of all that LangChain has to offer, but a\\xa0 curated list to call out some of its best features.Communicating with ModelsWhen building AI applications you’ll need a way to communicate with a model.', metadata={'description': 'In this blog, we cover LangChain: what it is, how it works, examples, & its ability to enhance applications by combining LLMs with other computational sources.', 'language': 'en', 'source': 'https://www.datastax.com/guides/what-is-langchain', 'title': 'What is LangChain? Getting Started with LangChain | DataStax'}),\n Document(page_content='up and deploy Astra DB on one of the world’s highest performing vector stores, built on Apache Cassandra, which is designed for handling massive volumes of data at scale. To get started for free register here.Subscribe to the RSS FeedShareJUMP TO SECTIONWhat is LangChain?Why is LangChain Important?What are the Key Components of LangChain?LLMs (Large Language Models)Prompt TemplatesIndexesRetrieversOutput ParsersVector StoreAgentsWhat are the Main Features of LangChain?Communicating with', metadata={'description': 'In this blog, we cover LangChain: what it is, how it works, examples, & its ability to enhance applications by combining LLMs with other computational sources.', 'language': 'en', 'source': 'https://www.datastax.com/guides/what-is-langchain', 'title': 'What is LangChain? Getting Started with LangChain | DataStax'})]"},"metadata":{}}]},{"cell_type":"code","source":"# using MMR\n\nmmr_docs = db.max_marginal_relevance_search(\n    query=query,\n    k=2,\n    fetch_k=2\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:43:04.712651Z","iopub.execute_input":"2024-05-07T17:43:04.713405Z","iopub.status.idle":"2024-05-07T17:43:04.736731Z","shell.execute_reply.started":"2024-05-07T17:43:04.713369Z","shell.execute_reply":"2024-05-07T17:43:04.735719Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"mmr_docs","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:43:08.487533Z","iopub.execute_input":"2024-05-07T17:43:08.488186Z","iopub.status.idle":"2024-05-07T17:43:08.494045Z","shell.execute_reply.started":"2024-05-07T17:43:08.488142Z","shell.execute_reply":"2024-05-07T17:43:08.493122Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='completion of the prompt.All the steps in the RAG pattern need to happen in succession. If anything errors along the way, the processing should stop. LangChain offers the “chain” construct to attach steps together in a specific way with a specific configuration. All LangChain libraries follow this chain construct to make it very easy to move steps around and create powerful processing pipelines.What are the Benefits of Using LangChain?Using LangChain offers several benefits in the realm of', metadata={'description': 'In this blog, we cover LangChain: what it is, how it works, examples, & its ability to enhance applications by combining LLMs with other computational sources.', 'language': 'en', 'source': 'https://www.datastax.com/guides/what-is-langchain', 'title': 'What is LangChain? Getting Started with LangChain | DataStax'}),\n Document(page_content='in LangChain are the decision-making components. They determine the best course of action based on the input, the context, and the available resources within the system.What are the Main Features of LangChain?Below is a summary of the most popular features of LangChain. This is not an exhaustive list of all that LangChain has to offer, but a\\xa0 curated list to call out some of its best features.Communicating with ModelsWhen building AI applications you’ll need a way to communicate with a model.', metadata={'description': 'In this blog, we cover LangChain: what it is, how it works, examples, & its ability to enhance applications by combining LLMs with other computational sources.', 'language': 'en', 'source': 'https://www.datastax.com/guides/what-is-langchain', 'title': 'What is LangChain? Getting Started with LangChain | DataStax'})]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Compressor Technique","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor","metadata":{"execution":{"iopub.status.busy":"2024-05-06T21:18:15.308441Z","iopub.execute_input":"2024-05-06T21:18:15.308856Z","iopub.status.idle":"2024-05-06T21:18:16.084301Z","shell.execute_reply.started":"2024-05-06T21:18:15.308825Z","shell.execute_reply":"2024-05-06T21:18:16.082957Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# meta_data_field_info = [\n#     AttributeInfo(\n#         name='source',\n#         description='Text chunks should be one of `/docs/use_cases/query_analysis/`',\n#         type='string'\n#     ),\n#     AttributeInfo(\n#         name='source',\n#         description='The heading from contexts',\n#         type='string'\n#     )\n# ]","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:14:21.279805Z","iopub.execute_input":"2024-05-06T19:14:21.280368Z","iopub.status.idle":"2024-05-06T19:14:21.284385Z","shell.execute_reply.started":"2024-05-06T19:14:21.280337Z","shell.execute_reply":"2024-05-06T19:14:21.283553Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# docs_content_disc = 'Langchain RAG'\n\n# ret = SelfQueryRetriever.from_llm(\n#     llm,\n#     db,\n#     docs_content_disc,\n#     meta_data_field_info,\n#     verbose=True\n# )","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:01:28.170588Z","iopub.execute_input":"2024-05-05T18:01:28.171279Z","iopub.status.idle":"2024-05-05T18:01:28.175321Z","shell.execute_reply.started":"2024-05-05T18:01:28.171244Z","shell.execute_reply":"2024-05-05T18:01:28.174301Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def pretty_print_docs(docs):\n    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:14:24.653864Z","iopub.execute_input":"2024-05-06T19:14:24.654351Z","iopub.status.idle":"2024-05-06T19:14:24.659272Z","shell.execute_reply.started":"2024-05-06T19:14:24.654318Z","shell.execute_reply":"2024-05-06T19:14:24.658341Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"compressor = LLMChainExtractor.from_llm(\n    llm=llm\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:14:25.962323Z","iopub.execute_input":"2024-05-06T19:14:25.963325Z","iopub.status.idle":"2024-05-06T19:14:25.968951Z","shell.execute_reply.started":"2024-05-06T19:14:25.963286Z","shell.execute_reply":"2024-05-06T19:14:25.967819Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"compressor_ret = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=db.as_retriever(\n        search_type = 'mmr'\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:14:27.881759Z","iopub.execute_input":"2024-05-06T19:14:27.882442Z","iopub.status.idle":"2024-05-06T19:14:27.887180Z","shell.execute_reply.started":"2024-05-06T19:14:27.882405Z","shell.execute_reply":"2024-05-06T19:14:27.886083Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"question = \"What is the langchain?\"","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:14:37.378934Z","iopub.execute_input":"2024-05-06T19:14:37.379761Z","iopub.status.idle":"2024-05-06T19:14:37.383630Z","shell.execute_reply.started":"2024-05-06T19:14:37.379725Z","shell.execute_reply":"2024-05-06T19:14:37.382669Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# compressed_docs = compressor_ret.get_relevant_documents(\n#     query=question\n# )","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:15:19.507064Z","iopub.execute_input":"2024-05-06T19:15:19.507466Z","iopub.status.idle":"2024-05-06T19:15:19.511698Z","shell.execute_reply.started":"2024-05-06T19:15:19.507435Z","shell.execute_reply":"2024-05-06T19:15:19.510864Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"pretty_print_docs(compressed_docs)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:15:22.611941Z","iopub.execute_input":"2024-05-06T19:15:22.612604Z","iopub.status.idle":"2024-05-06T19:15:22.656168Z","shell.execute_reply.started":"2024-05-06T19:15:22.612570Z","shell.execute_reply":"2024-05-06T19:15:22.654980Z"},"trusted":true},"execution_count":55,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pretty_print_docs(\u001b[43mcompressed_docs\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'compressed_docs' is not defined"],"ename":"NameError","evalue":"name 'compressed_docs' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# LLM + RAG","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:06:55.418674Z","iopub.execute_input":"2024-05-07T18:06:55.419045Z","iopub.status.idle":"2024-05-07T18:06:55.423451Z","shell.execute_reply.started":"2024-05-07T18:06:55.419013Z","shell.execute_reply":"2024-05-07T18:06:55.422529Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# this function is for clear chromadb\n\nimport shutil\n\ndef delete_all(directory):\n    try:\n        shutil.rmtree(directory)\n        print(f\"All files and subdirectories in {directory} have been deleted.\")\n    except Exception as e:\n        print(f\"Failed to delete {directory}: {e}\")\n\n# Usage\ndirectory_path = '/kaggle/working/docs/'\ndelete_all(directory_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:06:29.108117Z","iopub.execute_input":"2024-05-05T18:06:29.108779Z","iopub.status.idle":"2024-05-05T18:06:29.115491Z","shell.execute_reply.started":"2024-05-05T18:06:29.108747Z","shell.execute_reply":"2024-05-05T18:06:29.114486Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"All files and subdirectories in /kaggle/working/docs/ have been deleted.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create prompt template for LLM\n\ntemplate = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \nUse three sentences maximum. Keep the answer as concise as possible.\nAlways say \"thanks for asking!\" at the end of the answer. \n\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:16:53.873102Z","iopub.execute_input":"2024-05-07T18:16:53.874141Z","iopub.status.idle":"2024-05-07T18:16:53.879192Z","shell.execute_reply.started":"2024-05-07T18:16:53.874095Z","shell.execute_reply":"2024-05-07T18:16:53.878053Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"qa_chain_prompt = PromptTemplate.from_template(\n    template=template\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:16:55.597234Z","iopub.execute_input":"2024-05-07T18:16:55.597594Z","iopub.status.idle":"2024-05-07T18:16:55.602145Z","shell.execute_reply.started":"2024-05-07T18:16:55.597562Z","shell.execute_reply":"2024-05-07T18:16:55.601136Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"# Add memory","metadata":{}},{"cell_type":"code","source":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.memory import ConversationBufferWindowMemory","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:17:01.879637Z","iopub.execute_input":"2024-05-07T18:17:01.880011Z","iopub.status.idle":"2024-05-07T18:17:01.884569Z","shell.execute_reply.started":"2024-05-07T18:17:01.879978Z","shell.execute_reply":"2024-05-07T18:17:01.883541Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"mem = ConversationBufferMemory(\n     memory_key='chat_history', \n    return_messages=True, \n    human_prefix='Human',\n    ai_prefix='AI',\n    input_key='question',\n    verbose=False,\n)\nwindow_mem = ConversationBufferWindowMemory(\n    k=5,\n    memory_key='chat_history', \n    return_messages=True, \n    output_key='answer',\n    verbose=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:17:04.009247Z","iopub.execute_input":"2024-05-07T18:17:04.009578Z","iopub.status.idle":"2024-05-07T18:17:04.015067Z","shell.execute_reply.started":"2024-05-07T18:17:04.009553Z","shell.execute_reply":"2024-05-07T18:17:04.014008Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"qa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type='stuff',\n    retriever = db.as_retriever(\n        search_type='mmr',\n        search_kwargs={'k': 3, 'fetch_k': 50}\n    ),\n    return_source_documents=False,\n    chain_type_kwargs={\n        \"prompt\": qa_chain_prompt,\n        \"verbose\": False,\n        \"memory\": mem\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:17:05.878752Z","iopub.execute_input":"2024-05-07T18:17:05.879112Z","iopub.status.idle":"2024-05-07T18:17:05.885132Z","shell.execute_reply.started":"2024-05-07T18:17:05.879081Z","shell.execute_reply":"2024-05-07T18:17:05.884026Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"question = \"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:22:07.247997Z","iopub.execute_input":"2024-05-07T18:22:07.248706Z","iopub.status.idle":"2024-05-07T18:22:07.253079Z","shell.execute_reply.started":"2024-05-07T18:22:07.248672Z","shell.execute_reply":"2024-05-07T18:22:07.251993Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"result = qa_chain.invoke({'query':question})","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:22:08.412023Z","iopub.execute_input":"2024-05-07T18:22:08.412799Z","iopub.status.idle":"2024-05-07T18:22:14.246498Z","shell.execute_reply.started":"2024-05-07T18:22:08.412765Z","shell.execute_reply":"2024-05-07T18:22:14.245465Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(result['result'])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:22:16.824615Z","iopub.execute_input":"2024-05-07T18:22:16.825395Z","iopub.status.idle":"2024-05-07T18:22:16.829817Z","shell.execute_reply.started":"2024-05-07T18:22:16.825359Z","shell.execute_reply":"2024-05-07T18:22:16.828950Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":" llamaIndex is an open-source AI application that uses a neural network in real-time to process data and provide intelligent responses to user queries. It is built using the RAG pattern and includes a pipeline that involves creating an embedding from the user's question, looking up context from a vector database, creating a prompt, and submitting it to a large language model (LLM) for completion. Thanks for asking!\n","output_type":"stream"}]},{"cell_type":"code","source":"while True:\n    question = input(\"Ask: \")\n    result = qa_chain.invoke({'query':question})\n    print(result[\"result\"])\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:59:58.458027Z","iopub.execute_input":"2024-05-07T17:59:58.458704Z","iopub.status.idle":"2024-05-07T18:01:37.482672Z","shell.execute_reply.started":"2024-05-07T17:59:58.458671Z","shell.execute_reply":"2024-05-07T18:01:37.481356Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdin","text":"Ask:  What we need this framework?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" We need this framework because it is accurate and versatile. It can be used across various sectors and provides a lot of power by being able to handle diverse language tasks.\n\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask:  Can you give me any code example if avaible?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" I'm sorry, I don't have any code examples available. However, I can provide you with more information about LangChain and its capabilities. LangChain is a framework that can be used to build generative AI applications. It offers a few different scenarios that implement the Retrieval-Augmented Generation (RAG) pattern, such as a simple question/answer example, a RAG over code example, and a Chat History example. Output parsers make quick work of this by offering a deep integration between how the LLM will respond and the application’s custom classes. Learn more about output parsers. When a model is created, it’s trained with some data set. A model is considered small or large based on the amount of data that was used during training. For example, LLMs like GPT have been trained on the entire public internet. That amount of data deserves the “Large” label. The challenge is that no matter how large the model, it can still be limited by the quality and quantity of the training data. To address this, LangChain offers data retrieval capabilities that allow you to easily retrieve and incorporate data from various sources into your AI applications. Thanks for asking!\n\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     result \u001b[38;5;241m=\u001b[39m qa_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m:question})\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}]},{"cell_type":"code","source":"print(mem.buffer)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T18:15:40.216977Z","iopub.execute_input":"2024-05-07T18:15:40.217371Z","iopub.status.idle":"2024-05-07T18:15:40.222759Z","shell.execute_reply.started":"2024-05-07T18:15:40.217336Z","shell.execute_reply":"2024-05-07T18:15:40.221681Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"[HumanMessage(content='What is important features in langchain. tell me in bullets.'), AIMessage(content='\\n\\n* Communicating with Models: LangChain allows you to communicate with models in real-time, without the need for manual input or note-taking.\\n* Contextual Awareness: LangChain can understand the context of the conversation and adjust its responses accordingly, making it more productive and enjoyable to use.\\n* Integration with Popular Tools: LangChain can integrate with popular tools such as Apache, Apache Cassandra, Cassandra, Apache Pulsar, and Pulsar, making it easy to use and deploy.\\n\\nThanks for asking!'), HumanMessage(content='What is LAW Degree?'), AIMessage(content=' A LAW degree is a legal degree that is typically earned by students who want to become lawyers. It is a graduate degree that is offered by law schools and is designed to provide students with the knowledge and skills they need to practice law. A LAW degree typically includes courses in subjects such as contracts, civil procedure, torts, and constitutional law. It is important to note that a LAW degree is not a requirement for practicing law in the United States, but it is highly recommended for those who want to become lawyers. Thanks for asking!')]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}